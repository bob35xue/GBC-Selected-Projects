{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set Up the Snowflake Connector in PySpark:\n",
    "You need to install the Snowflake Spark Connector to read data from Snowflake into PySpark.\n",
    "pip install pyspark\n",
    "pip install snowflake-connector-python\n",
    "pip install snowflake-sqlalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the Snowflake Spark Connector in my PySpark environment.\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession,SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize the Spark session with Snowflake connector\n",
    "# jdbc_driver_path = \"D:\\Box\\Apps\\Spark\\spark-3.5.3-bin-hadoop3\\spark-snowflake_2.12-2.10.1-spark_3.1.jar,D:\\Box\\Apps\\Spark\\spark-3.5.3-bin-hadoop3\\snowflake-jdbc-3.13.31.jar\"\n",
    "# .config(\"spark.jars.packages\", \"net.snowflake:spark-snowflake_2.12:2.9.2\") \\\n",
    "#    .config(\"spark.jars.packages\", \"net.snowflake:snowflake-jdbc:3.13.14\") \\\n",
    "# \"sfWarehouse\" : \"<your_snowflake_warehouse>\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TPCDS_SF10TCL Churn Analysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"net.snowflake:spark-snowflake_2.12:2.10.1-spark_3.1\") \\\n",
    "    .config(\"spark.jars.packages\", \"net.snowflake:snowflake-jdbc:3.13.13\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Snowflake connection options\n",
    "sfOptions = {\n",
    "    \"sfURL\" : \"https://pdxuwmc-wb15506.snowflakecomputing.com\",\n",
    "    \"sfUser\" : \"Bob35\",\n",
    "    \"sfPassword\" : \"*******\",\n",
    "    \"sfDatabase\" : \"SNOWFLAKE_SAMPLE_DATA\",\n",
    "    \"sfSchema\" : \"TPCDS_SF10TCL\",\n",
    "    \"sfRole\" : \"ACCOUNTADMIN\", \n",
    "    \n",
    "}\n",
    "\n",
    "# Function to read Snowflake tables\n",
    "def read_snowflake_table(table_name):\n",
    "    return spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**sfOptions) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load the Tables from TPCDS_SF10TCL:\n",
    "Now, we will load the specific tables (STORE_SALES, WEB_SALES, STORE_RETURNS, WEB_RETURNS, CUSTOMER, PROMOTION) from TPCDS_SF10TCL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tables from TPCDS_SF10TCL\n",
    "store_sales = read_snowflake_table(\"STORE_SALES\")\n",
    "web_sales = read_snowflake_table(\"WEB_SALES\")\n",
    "store_returns = read_snowflake_table(\"STORE_RETURNS\")\n",
    "web_returns = read_snowflake_table(\"WEB_RETURNS\")\n",
    "customer = read_snowflake_table(\"CUSTOMER\")\n",
    "promotions = read_snowflake_table(\"PROMOTION\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Adjust Feature Extraction for TPCDS_SF10TCL:\n",
    "Adjust the feature extraction to work with the specific schema. The logic stays the same as the previous explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.show of DataFrame[ws_bill_customer_sk: decimal(38,0), purchase_count_web: bigint]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, avg, when\n",
    "\n",
    "# 1. Purchase Frequency: count number of transactions per customer\n",
    "store_sales_freq = store_sales.groupBy(\"ss_customer_sk\") \\\n",
    "    .agg(count(\"ss_ticket_number\").alias(\"purchase_count_store\"))\n",
    "\n",
    "store_sales_freq.show\n",
    "\n",
    "web_sales_freq = web_sales.groupBy(\"ws_bill_customer_sk\") \\\n",
    "    .agg(count(\"ws_order_number\").alias(\"purchase_count_web\"))\n",
    "\n",
    "web_sales_freq.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Average Purchase Value: calculate avg value per transaction\n",
    "# store_avg_purchase = store_sales.groupBy(\"ss_customer_sk\") \\\n",
    "#     .agg(avg(\"ss_net_paid\").alias(\"avg_store_purchase_value\"))\n",
    "# Step 2: Join store_returns with store_sales_freq to get purchase counts for each customer\n",
    "# We use sr_return_quantity to represent the number of returns\n",
    "store_returns_agg = store_returns.join(store_sales_freq, store_returns[\"sr_customer_sk\"] == store_sales_freq[\"ss_customer_sk\"], \"left\")\n",
    "# store_returns_agg = store_returns.join(store_sales_freq, store_returns[\"sr_customer_sk\"] == store_sales_freq[\"ss_customer_sk\"], \"left\")\n",
    "\n",
    "# web_avg_purchase = web_sales.groupBy(\"ws_bill_customer_sk\") \\\n",
    "#     .agg(avg(\"ws_net_paid\").alias(\"avg_web_purchase_value\"))\n",
    "\n",
    "web_returns_agg = web_returns.join(web_sales_freq, web_returns[\"wr_returning_customer_sk\"] == web_sales_freq[\"ws_bill_customer_sk\"], \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Return Rate: Join store_returns with store_sales_freq to get purchase counts for return rate calculation\n",
    "# store_returns_agg = store_returns.join(store_sales_freq, store_returns[\"sr_customer_sk\"] == store_sales_freq[\"ss_customer_sk\"], \"left\") \\\n",
    "#    .groupBy(\"sr_customer_sk\") \\\n",
    "#    .agg((count(\"sr_ticket_number\") / col(\"purchase_count_store\")).alias(\"store_return_rate\"))\n",
    "\n",
    "# store_returns_agg = store_returns_agg.groupBy(\"sr_customer_sk\") \\\n",
    "#    .agg(\n",
    "#        (count(\"sr_ticket_number\") / max(\"purchase_count_store\")).alias(\"store_return_rate\")  # Use max() or first() to aggregate purchase_count_store\n",
    "#    )\n",
    "store_returns_agg = store_returns_agg.groupBy(\"sr_customer_sk\") \\\n",
    "    .agg(\n",
    "        (sum(\"sr_return_quantity\") / max(\"purchase_count_store\")).alias(\"store_return_rate\")  # Use sum() of sr_return_quantity to get total returns\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "#web_returns_agg = web_returns.join(web_sales_freq, web_returns[\"wr_returning_customer_sk\"] == web_sales_freq[\"ws_bill_customer_sk\"], \"left\") \\\n",
    "#    .groupBy(\"wr_returning_customer_sk\") \\\n",
    "#    .agg((count(\"wr_order_number\") / col(\"purchase_count_web\")).alias(\"web_return_rate\"))\n",
    "\n",
    "web_returns_agg = web_returns_agg.groupBy(\"wr_returning_customer_sk\") \\\n",
    "    .agg(\n",
    "        (sum(\"wr_return_quantity\") / max(\"purchase_count_web\")).alias(\"web_return_rate\")  # Use sum() of wr_return_quantity for web returns\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Promotional Participation: check if customers participated in a promotion\n",
    "promo_sales_store = store_sales.join(promotions, store_sales[\"ss_promo_sk\"] == promotions[\"p_promo_sk\"]) \\\n",
    "    .groupBy(\"ss_customer_sk\").agg(count(\"ss_ticket_number\").alias(\"store_promo_participation\"))\n",
    "\n",
    "promo_sales_web = web_sales.join(promotions, web_sales[\"ws_promo_sk\"] == promotions[\"p_promo_sk\"]) \\\n",
    "    .groupBy(\"ws_bill_customer_sk\").agg(count(\"ws_order_number\").alias(\"web_promo_participation\"))\n",
    "\n",
    "# Merge all features together with customer demographics\n",
    "# Using coalesce to manage potential nulls and any_value() to prevent non-aggregated issues\n",
    "\n",
    "# Join customer with customer_demographics to get demographic details (like gender)\n",
    "customer_demographics = read_snowflake_table(\"customer_demographics\")  # Assuming this is the correct table name\n",
    "\n",
    "# Join customer and customer_demographics on the c_current_cdemo_sk key\n",
    "customer_with_demo = customer.join(customer_demographics, customer[\"c_current_cdemo_sk\"] == customer_demographics[\"cd_demo_sk\"], \"left\")\n",
    "\n",
    "# customer_income_band = read_snowflake_table(\"income_band\")\n",
    "\n",
    "\n",
    "\n",
    "# Now proceed with joining the rest of the tables\n",
    "features = customer_with_demo.join(store_sales_freq, customer_with_demo[\"c_customer_sk\"] == store_sales_freq[\"ss_customer_sk\"], \"left\") \\\n",
    "    .join(web_sales_freq, customer_with_demo[\"c_customer_sk\"] == web_sales_freq[\"ws_bill_customer_sk\"], \"left\") \\\n",
    "    .join(store_avg_purchase, customer_with_demo[\"c_customer_sk\"] == store_avg_purchase[\"ss_customer_sk\"], \"left\") \\\n",
    "    .join(web_avg_purchase, customer_with_demo[\"c_customer_sk\"] == web_avg_purchase[\"ws_bill_customer_sk\"], \"left\") \\\n",
    "    .join(store_returns_agg, customer_with_demo[\"c_customer_sk\"] == store_returns_agg[\"sr_customer_sk\"], \"left\") \\\n",
    "    .join(web_returns_agg, customer_with_demo[\"c_customer_sk\"] == web_returns_agg[\"wr_returning_customer_sk\"], \"left\") \\\n",
    "    .join(promo_sales_store, customer_with_demo[\"c_customer_sk\"] == promo_sales_store[\"ss_customer_sk\"], \"left\") \\\n",
    "    .join(promo_sales_web, customer_with_demo[\"c_customer_sk\"] == promo_sales_web[\"ws_bill_customer_sk\"], \"left\") \\\n",
    "    .select(customer_with_demo[\"c_customer_sk\"], \n",
    "            \"purchase_count_store\", \n",
    "            \"purchase_count_web\",\n",
    "            \"avg_store_purchase_value\", \n",
    "            \"avg_web_purchase_value\",\n",
    "            \"store_return_rate\", \n",
    "            \"web_return_rate\",\n",
    "            \"store_promo_participation\", \n",
    "            \"web_promo_participation\",\n",
    "            \"c_birth_year\", \n",
    "            \"cd_gender\" )  # Updated to use the gender column from customer_demographics\n",
    "           # \"c_income_band_sk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Add Churn Label: customers who havenâ€™t made a purchase in a long time (e.g., last 6 months)\n",
    "# Add appropriate column for churn labeling\n",
    "latest_store_purchase = store_sales.agg(max(\"ss_sold_date_sk\")).collect()[0][0]\n",
    "latest_web_purchase = web_sales.agg(max(\"ws_sold_date_sk\")).collect()[0][0]\n",
    "latest_purchase_date = max(latest_store_purchase, latest_web_purchase)\n",
    "\n",
    "# Label customers as churned if they have not purchased in the last 180 days\n",
    "features = features.withColumn(\"is_churn\", when(\n",
    "    (col(\"purchase_count_store\").isNull() & col(\"purchase_count_web\").isNull()) |\n",
    "    (col(\"purchase_count_store\") < lit(latest_purchase_date - 180)) & \n",
    "    (col(\"purchase_count_web\") < lit(latest_purchase_date - 180)),\n",
    "    lit(1)\n",
    ").otherwise(lit(0)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Train Churn Prediction Model: Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define feature columns\n",
    "feature_columns = [\"purchase_count_store\", \"purchase_count_web\", \"avg_store_purchase_value\", \n",
    "                   \"avg_web_purchase_value\", \"store_return_rate\", \"web_return_rate\", \n",
    "                   \"store_promo_participation\", \"web_promo_participation\"]\n",
    "\n",
    "# VectorAssembler to combine feature columns into a single vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Define the logistic regression model\n",
    "lr = LogisticRegression(labelCol=\"is_churn\", featuresCol=\"features\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "# Fit the model\n",
    "model = pipeline.fit(features)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.transform(features)\n",
    "predictions.select(\"c_customer_sk\", \"is_churn\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Adjustments for TPCDS_SF10TCL:\n",
    "We used the TPCDS_SF10TCL schema for all the table references.\n",
    "The rest of the logic remains the same, where we aggregate features from customer data, calculate churn labels, and train a predictive model.\n",
    "5. Churn Labeling:\n",
    "For the churn label, customers who haven't made a purchase in the last 180 days are labeled as churned. This logic can be adjusted to fit specific business requirements.\n",
    "\n",
    "6. Final Considerations:\n",
    "This approach helps extract key customer features and build a predictive churn model using TPC-DS data. We can refine the model by tuning the feature set and model parameters further, or try other classification algorithms like Random Forest, Gradient Boosted Trees, etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
